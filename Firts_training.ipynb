{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Firts_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw2SmNljFVPM",
        "outputId": "8e03c5b1-ec5f-440a-8605-2ccdd54cbe4d"
      },
      "source": [
        "pip install tf-agents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/cd/a0710b1caae042b7a4d54fc74073fb4df7adf073934798443bdc0059813a/tf_agents-0.7.1-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.12.4)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.19.5)\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-probability>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.3->tf-agents) (56.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.1.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Installing collected packages: tf-agents\n",
            "Successfully installed tf-agents-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYrihmSHNiXM"
      },
      "source": [
        "\n",
        "from collections import deque\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from tf_agents.networks import q_rnn_network\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "import tensorflow as tf\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "import os\n",
        "import tempfile\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.utils import common\n",
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAalzpX6OK93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47497e1-4eb8-42fb-e3c5-ebab41b8dfb3"
      },
      "source": [
        "### reading data from csv and saving to dicts\n",
        "\n",
        "\n",
        "days_prices = defaultdict(dict)\n",
        "counter = 0\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/totrain\"\n",
        "for files in os.listdir(path):\n",
        "    counter+=1\n",
        "    np_dict = defaultdict(dict)\n",
        "    prices = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/totrain/{files}\", index_col = 0)\n",
        "    prices = prices.drop(\"time\", axis=1)\n",
        "    l = len(prices.columns)\n",
        "    for k in range(0,l):\n",
        "        prices = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/totrain/{files}\", index_col = 0)\n",
        "        prices = prices.drop(\"time\", axis=1)\n",
        "        prices = prices.iloc[:, k:k+1]\n",
        "        for i in prices.columns:\n",
        "            prices[f\"mov_5{i}\"] = prices[f\"{i}\"].rolling(window=5).mean()\n",
        "            prices[f\"mov_20{i}\"] = prices[f\"{i}\"].rolling(window=20).mean()\n",
        "            prices[f\"mov_80{i}\"] = prices[f\"{i}\"].rolling(window=80).mean()\n",
        "            prices = prices.dropna()\n",
        "            prices[\"5-20\"] = np.round((prices[f\"mov_5{i}\"] - prices[f\"mov_20{i}\"])/prices[f\"{i}\"], 4)\n",
        "            prices[\"5-80\"] = np.round((prices[f\"mov_5{i}\"] - prices[f\"mov_80{i}\"])/prices[f\"{i}\"], 4)\n",
        "            prices[\"20-80\"] = np.round((prices[f\"mov_20{i}\"] - prices[f\"mov_80{i}\"])/prices[f\"{i}\"], 4)\n",
        "            np_dict[f\"np_diff_{i}\"] = np.array([prices[\"5-20\"].values.reshape(-1,1), prices[\"5-80\"].values.reshape(-1,1), prices[\"20-80\"].values.reshape(-1,1), prices[f\"{i}\"].values.reshape(-1,1) ])\n",
        "    days_prices[f\"np_dict_{counter}\"] = np_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4ED5_DbOrZ5",
        "outputId": "b2bd4661-af72-40d5-aa14-1986392dceb0"
      },
      "source": [
        "### defining max length of training steps\n",
        "\n",
        "\n",
        "def max_steps_lenght(days_prices):\n",
        "    max_steps = 0\n",
        "    for i in days_prices.keys():\n",
        "        crypto_dict = days_prices[i].keys()\n",
        "        number_of_crypto = len(crypto_dict)\n",
        "        first_dict = list(crypto_dict)[0]\n",
        "        lenght = len(days_prices[i][\"np_diff_XLMUSDT\"][0])-60\n",
        "        lenght = lenght * (number_of_crypto) # timestamp\n",
        "        max_steps += lenght\n",
        "    return max_steps\n",
        "max_steps_lenght(days_prices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2279040"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH6eCSqRiTjM"
      },
      "source": [
        "### creating environment\n",
        "\n",
        "\n",
        "class CustomEnv(py_environment.PyEnvironment):\n",
        "\n",
        "    def __init__(self, days_price, initial_balance, lookback_window_size):\n",
        "\n",
        "        self.lookback_window_size = lookback_window_size\n",
        "\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(self.lookback_window_size * 3 + 1,), dtype=np.float32, name='observation')\n",
        "\n",
        "        self.days_price = days_price\n",
        "\n",
        "        self.days_count = 1 \n",
        "\n",
        "        self.price_dict = days_price[f\"np_dict_{self.days_count}\"]\n",
        "\n",
        "        self.dict_names = list(self.price_dict.keys())\n",
        "\n",
        "        self.dict_count = 0\n",
        "\n",
        "        self.initial_balance = initial_balance\n",
        "\n",
        "        self.last_step = len(self.price_dict[f\"{self.dict_names[self.dict_count]}\"][0]) - self.lookback_window_size\n",
        "\n",
        "        self.episode_ended = False\n",
        "\n",
        "        self.active_position = 0\n",
        "\n",
        "        self.action = 0\n",
        "\n",
        "        self.fees = 0.001\n",
        "\n",
        "        self.spread = 0.001\n",
        "\n",
        "        self.money = initial_balance\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.cum_reward = 0\n",
        "\n",
        "        self.crypto_held = 0\n",
        "\n",
        "        self.previous_money = initial_balance\n",
        "\n",
        "        self._state = False\n",
        "        \n",
        "        self.price_bought = 0\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        self.episode_ended = False\n",
        "        self.current_step = 0\n",
        "        self.price_bought = 0\n",
        "        self.money = self.initial_balance\n",
        "        self.active_position = 0\n",
        "        self.action = 0\n",
        "        self.crypto_held = 0\n",
        "        self.dict_count += 1\n",
        "        if self.dict_count == len(self.dict_names):\n",
        "            self.days_count += 1\n",
        "            self.dict_count = 0\n",
        "            self.price_dict = self.days_price[f\"np_dict_{self.days_count}\"]\n",
        "            self.dict_names = list(self.price_dict.keys())\n",
        "            self.last_step = len(self.price_dict[f\"{self.dict_names[self.dict_count]}\"][0]) - self.lookback_window_size\n",
        "        self.mov_price = self.price_dict[f\"{self.dict_names[self.dict_count]}\"]\n",
        "        # try:\n",
        "        #   self.mov_price = self.price_dict[f\"{self.dict_names[self.dict_count]}\"]\n",
        "        # except IndexError:\n",
        "        #     self.days_count += 1\n",
        "        #     self.dict_count = 0\n",
        "        #     self.price_dict = self.days_price[f\"np_dict_{self.days_count}\"]\n",
        "        #     self.dict_names = list(self.price_dict.keys())\n",
        "        #     self.last_step = len(self.price_dict[f\"{self.dict_names[self.dict_count]}\"][0]) - self.lookback_window_size\n",
        "        self.previous_money = self.initial_balance\n",
        "        self.cum_reward = 0  ###Only now\n",
        "        self.pnl = 0\n",
        "\n",
        "        self.last = self.current_step + self.lookback_window_size\n",
        "\n",
        "        self._state = np.array(\n",
        "            [self.mov_price[0, self.current_step:self.last], self.mov_price[1, self.current_step:self.last],\n",
        "             self.mov_price[2, self.current_step:self.last]])\n",
        "\n",
        "        self._state = np.append(self._state, self.active_position)\n",
        "\n",
        "        return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "    def open_position(self, action, current_price):\n",
        "        reward = 0\n",
        "        if (self.active_position == 0):\n",
        "            if action == 1:\n",
        "                self.active_position = 1\n",
        "                if self.money <= self.initial_balance * 0.8:\n",
        "                    self.episode_ended = True\n",
        "                    reward = -200\n",
        "                else:\n",
        "                    reward = 0\n",
        "                    self.money = self.money - (self.money * self.spread) - (self.money * self.fees)\n",
        "                    self.crypto_held = (self.money / current_price)\n",
        "                    self.price_bought = current_price\n",
        "                    \n",
        "        elif (self.active_position == 1):\n",
        "            if action == 0:\n",
        "                self.active_position = 0\n",
        "                self.money = self.crypto_held * current_price - (self.money * self.spread) - (self.money * self.fees)\n",
        "                reward = (current_price - self.price_bought) * self.crypto_held * 2\n",
        "                self.crypto_held = 0\n",
        "                self.price_bought = 0\n",
        "            else:\n",
        "                self.money = self.crypto_held * current_price\n",
        "                reward = 0\n",
        "\n",
        "        return reward  # think about money balance\n",
        "\n",
        "    def next_observation(self):\n",
        "\n",
        "        self.last = self.current_step + self.lookback_window_size\n",
        "        previous_price = self.mov_price[3, self.lookback_window_size + self.current_step - 1]\n",
        "        current_price = self.mov_price[3, self.lookback_window_size + self.current_step]\n",
        "        obs = np.array([self.mov_price[0, self.current_step:self.last], self.mov_price[1, self.current_step:self.last],\n",
        "                        self.mov_price[2, self.current_step:self.last]])\n",
        "        self.current_step += 1\n",
        "        return obs, current_price, previous_price \n",
        "\n",
        "    def _step(self, action):\n",
        "        reward = 0\n",
        "        done = False\n",
        "        if self.episode_ended == True:\n",
        "            return self._reset()\n",
        "        obs, current_price, previous_price = self.next_observation()\n",
        "        self._state = obs\n",
        "        if (action == 1):\n",
        "            self.action = action\n",
        "            reward = self.open_position(action, current_price)\n",
        "\n",
        "        elif (action == 0):\n",
        "            if self.active_position == 0:\n",
        "                reward = -2\n",
        "            else:\n",
        "                self.action = action\n",
        "                reward = self.open_position(action, current_price)\n",
        "\n",
        "        if self.last == self.last_step: ##################################################################check should be if last = last.step -1\n",
        "            self.episode_ended = True\n",
        "\n",
        "        if self.episode_ended == True:\n",
        "            done = True\n",
        "            self.money = self.crypto_held * current_price - (self.money * self.spread) - (self.money * self.fees)\n",
        "            self.active_position = 0\n",
        "            reward = self.money - self.initial_balance\n",
        "\n",
        "        pnl = (self.money - self.previous_money)\n",
        "        self.previous_money = self.money\n",
        "        self.pnl += pnl\n",
        "        info = {}\n",
        "        self._state = np.append(self._state, self.active_position)\n",
        "        # return self._state, reward, done, info\n",
        "        if self.episode_ended == True:\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), float(reward))\n",
        "        else:\n",
        "            return ts.transition(\n",
        "                np.array(self._state, dtype=np.float32), reward=float(reward), discount=0)\n",
        "\n",
        "    def render(self):\n",
        "        print(f'Step: {self.current_step}, Money: {self.money}, PnL{self.pnl}, Cum reward{self.cum_reward}')\n",
        "    #####SKALIBROWAĆ REWARD\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TgV3mmmEqv7"
      },
      "source": [
        "#### creating model and agent\n",
        "\n",
        "\n",
        "class Agent_net():\n",
        "\n",
        "    def __init__(self, env):\n",
        "        # defining hyperparameters\n",
        "        self.env = env\n",
        "\n",
        "        self.train_env = tf_py_environment.TFPyEnvironment(self.env)\n",
        "        self.eval_env = tf_py_environment.TFPyEnvironment(self.env)\n",
        "        # network configuration\n",
        "        self.input_fc_layer_params = (181,)\n",
        "        self.lstm_size = (60,)\n",
        "        self.output_fc_layer_params = (60,)\n",
        "        self.learning_rate = 0.005\n",
        "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "        self.td_errors_loss_fn = common.element_wise_squared_loss\n",
        "        self.train_step_counter = tf.compat.v1.train.get_or_create_global_step() #tf.Variable(0)  \n",
        "        self.replay_buffer_max_length = 1000000\n",
        "        self.batch_size = 64\n",
        "        self.epsilon = 0.2\n",
        "\n",
        "    def qrnn(self):\n",
        "        # create a q_RNNnet\n",
        "        self.q_net = q_rnn_network.QRnnNetwork(\n",
        "            self.train_env.observation_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            input_fc_layer_params=self.input_fc_layer_params,\n",
        "            lstm_size=self.lstm_size,\n",
        "            output_fc_layer_params=self.output_fc_layer_params)\n",
        "\n",
        "    def agent_create(self):\n",
        "        self.qrnn()\n",
        "        self.target = self.qrnn()\n",
        "        self.agent = dqn_agent.DqnAgent(\n",
        "            self.train_env.time_step_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            q_network=self.q_net,\n",
        "            #target_q_network = self.target,\n",
        "            #target_update_tau = 0.1,\n",
        "            #target_update_period = 100,\n",
        "            epsilon_greedy = self.epsilon,\n",
        "            optimizer=self.optimizer,\n",
        "            td_errors_loss_fn=self.td_errors_loss_fn,\n",
        "            train_step_counter=self.train_step_counter)\n",
        "\n",
        "        self.agent.initialize() # just to start agent\n",
        "        self.eval_policy = self.agent.policy # return the policy\n",
        "        self.random_policy = random_tf_policy.RandomTFPolicy(self.train_env.time_step_spec(),\n",
        "                                                             self.train_env.action_spec())\n",
        "\n",
        "    def r_buffer(self):\n",
        "        self.agent_create()\n",
        "        self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "            data_spec=self.agent.collect_data_spec, # getting data scpecification \n",
        "            batch_size=self.train_env.batch_size, # how many experience there are in a batch\n",
        "            max_length=self.replay_buffer_max_length) # maximum capacity of buffer is max_lenght * batch size (stored in a tuple)\n",
        "\n",
        "    def iter_data(self):\n",
        "        dataset = self.replay_buffer.as_dataset(\n",
        "            num_parallel_calls=3, #how many process at time, if none sequential\n",
        "            sample_batch_size=self.batch_size, #how many batches to get\n",
        "            num_steps=10).prefetch(3) #how many steps in a batch\n",
        "\n",
        "        iterator = iter(dataset)\n",
        "        return (self.agent, self.train_env, self.replay_buffer, self.eval_env, iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wv7gdVxO33e"
      },
      "source": [
        "### steps collection for training\n",
        "\n",
        "\n",
        "\n",
        "def collect_step(environment, policy, buffer):\n",
        "    time_step = environment.current_time_step() # returns the current time_step and initializes the environment if needed\n",
        "    action_step = policy.action(time_step, policy.get_initial_state(batch_size=env.batch_size)) #works with 1, Generates the initial state for stateful policies ?\n",
        "    next_time_step = environment.step(action_step.action) # gets next time_step from taken action\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step) # change it to trajectory, which represents a sequence of aligned time steps\n",
        "\n",
        "    # Add trajectory to the replay buffer\n",
        "    buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJjWDh30O58x"
      },
      "source": [
        "### computing reward returned in order to work collect_step() must be first\n",
        "\n",
        "\n",
        "\n",
        "def compute_avg_return(environment, policy, num_eval_episodes):\n",
        "\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_eval_episodes): # number o evaluation episodes\n",
        "\n",
        "        time_step = environment.reset() # getting first time_step\n",
        "        episode_return = 0\n",
        "        policy_state = policy.get_initial_state(batch_size=env.batch_size)\n",
        "        avg_return = 0\n",
        "        \n",
        "\n",
        "        while not time_step.is_last():\n",
        "            action_step = policy.action(time_step, policy_state) # action taken for given timestep\n",
        "            time_step = environment.step(action_step.action) # assign next time_step \n",
        "            episode_return += time_step.reward # reward taken from next time_step ?\n",
        "           \n",
        "        total_return+= episode_return\n",
        "        print(episode_return)\n",
        "    avg_return = total_return  / num_eval_episodes\n",
        "    return avg_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAcomA01E4xc"
      },
      "source": [
        "### training\n",
        "\n",
        "\n",
        "def training(agent, train_env, replay_buffer, eval_env, iterator):\n",
        "    num_iterations = 1000000  \n",
        "    collect_steps_per_iteration = 10 \n",
        "    log_interval = 5000\n",
        "    eval_interval = 100000\n",
        "    num_eval_episodes = 3\n",
        "    index_out = False\n",
        "\n",
        "    try:\n",
        "        % % time # used for prolonged training\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "    agent.train = common.function(agent.train)\n",
        "\n",
        "    # Evaluate the agent's policy once before training.\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes) #use created function\n",
        "    returns = [avg_return]\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "\n",
        "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "        try:\n",
        "          for _ in range(collect_steps_per_iteration):\n",
        "              collect_step(train_env, agent.collect_policy, replay_buffer) # expanding buffer, collect_policy used to gather info target and local read more\n",
        "        except IndexError:\n",
        "          index_out = True   \n",
        "\n",
        "        # Sample a batch of data from the buffer and update the agent's network.\n",
        "        experience, unused_info = next(iterator) \n",
        "        train_loss = agent.train(experience).loss\n",
        "\n",
        "        step = agent.train_step_counter.numpy()\n",
        "\n",
        "        if step % log_interval == 0:\n",
        "            print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "            \n",
        "        if step % eval_interval == 0:\n",
        "            try:\n",
        "                avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "                print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "                returns.append(avg_return)\n",
        "            except Exception:\n",
        "                print(\"index error\")\n",
        "                index_out = True\n",
        "        if index_out == True:\n",
        "          break\n",
        "\n",
        "    return agent.train_step_counter, agent, replay_buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OJx_i18PB2W",
        "outputId": "f8dc00c7-3aa8-4eaa-e895-7f4150233277"
      },
      "source": [
        "### calling env, agent creation, training etc\n",
        "\n",
        "env = None\n",
        "tf.compat.v1.reset_default_graph()\n",
        "env = CustomEnv(days_prices, 1000, 60)\n",
        "na = Agent_net(env)\n",
        "na.r_buffer()\n",
        "agent, train_env, replay_buffer, eval_env, iterator = na.iter_data()\n",
        "save_train_step_counter, save_agent, save_replay_buffer = training(agent, train_env, replay_buffer, eval_env, iterator)\n",
        "\n",
        "chkpdir = \"/content/drive/MyDrive/Colab Notebooks/policies\"\n",
        "policy_dir = os.path.join(chkpdir, 'policy')\n",
        "checkpoint_dir = os.path.join(chkpdir, 'checkpoint')\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    model = na.q_net,\n",
        "    agent=save_agent,\n",
        "    policy=save_agent.policy,\n",
        "    replay_buffer=save_replay_buffer,\n",
        "    global_step=save_agent.train_step_counter\n",
        ")\n",
        "\n",
        "tf_policy_saver = policy_saver.PolicySaver(save_agent.policy)\n",
        "checkpoint = train_checkpointer.save(save_agent.train_step_counter)\n",
        "#train_checkpointer.initialize_or_restore()\n",
        "#global_step = tf.compat.v1.train.get_global_step()\n",
        "tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
            "tf.Tensor([68.26729], shape=(1,), dtype=float32)\n",
            "tf.Tensor([3.737951], shape=(1,), dtype=float32)\n",
            "tf.Tensor([27.077602], shape=(1,), dtype=float32)\n",
            "step = 5000: loss = 26563.287109375\n",
            "step = 10000: loss = 15929.7353515625\n",
            "step = 15000: loss = 35056.953125\n",
            "step = 20000: loss = 25634.2734375\n",
            "step = 25000: loss = 520.248779296875\n",
            "step = 30000: loss = 16435.771484375\n",
            "step = 35000: loss = 19494.599609375\n",
            "step = 40000: loss = 15192.013671875\n",
            "step = 45000: loss = 15668.3125\n",
            "step = 50000: loss = 171.50326538085938\n",
            "step = 55000: loss = 143.80511474609375\n",
            "step = 60000: loss = 5807.59619140625\n",
            "step = 65000: loss = 15778.8369140625\n",
            "step = 70000: loss = 15302.8271484375\n",
            "step = 75000: loss = 268.3473815917969\n",
            "step = 80000: loss = 15721.26171875\n",
            "step = 85000: loss = 15862.2724609375\n",
            "step = 90000: loss = 46505.32421875\n",
            "step = 95000: loss = 259.28289794921875\n",
            "step = 100000: loss = 15886.98046875\n",
            "tf.Tensor([-2713.6013], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-3020.6084], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-2962.4907], shape=(1,), dtype=float32)\n",
            "step = 100000: Average Return = [-2898.9004]\n",
            "step = 105000: loss = 15661.76171875\n",
            "step = 110000: loss = 434.9533996582031\n",
            "step = 115000: loss = 15475.369140625\n",
            "step = 120000: loss = 31473.4921875\n",
            "step = 125000: loss = 46381.12109375\n",
            "step = 130000: loss = 408.3161315917969\n",
            "step = 135000: loss = 514.761474609375\n",
            "step = 140000: loss = 15553.5927734375\n",
            "step = 145000: loss = 15253.77734375\n",
            "step = 150000: loss = 239.6143341064453\n",
            "step = 155000: loss = 15852.61328125\n",
            "step = 160000: loss = 207.13787841796875\n",
            "step = 165000: loss = 223.63253784179688\n",
            "step = 170000: loss = 107.66749572753906\n",
            "step = 175000: loss = 16241.640625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as QRnnNetwork_layer_call_and_return_conditional_losses, QRnnNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dynamic_unroll_1_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as QRnnNetwork_layer_call_and_return_conditional_losses, QRnnNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dynamic_unroll_1_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/policies/policy/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/policies/policy/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkVPodEJWDyR"
      },
      "source": [
        "chkpdir = \"/content/drive/MyDrive/Colab Notebooks/policies\"\n",
        "policy_dir = os.path.join(chkpdir, 'policy')\n",
        "checkpoint_dir = os.path.join(chkpdir, 'checkpoint')\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=agent.train_step_counter\n",
        ")\n",
        "\n",
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "checkpoint = train_checkpointer.save(agent.train_step_counter)\n",
        "#train_checkpointer.initialize_or_restore()\n",
        "#global_step = tf.compat.v1.train.get_global_step()\n",
        "tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKZh1bUlw4yn",
        "outputId": "237c0284-ea62-455d-9732-c0e788922ff5"
      },
      "source": [
        "agent.train_step_counter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'global_step:0' shape=() dtype=int64, numpy=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Kf0ADu2N31"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}