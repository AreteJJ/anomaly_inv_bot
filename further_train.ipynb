{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "further_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZP7fuTMKNpM",
        "outputId": "6176f198-f6f3-49ec-f52e-847e16443bc3"
      },
      "source": [
        "pip install tf-agents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/cd/a0710b1caae042b7a4d54fc74073fb4df7adf073934798443bdc0059813a/tf_agents-0.7.1-py3-none-any.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 15.2MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 20.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 15.1MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 10.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 4.4MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 307kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 737kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 808kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 880kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 952kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0MB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.2MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.3->tf-agents) (56.1.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.1.6)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Installing collected packages: tf-agents\n",
            "Successfully installed tf-agents-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ERz6DqVKSHt"
      },
      "source": [
        "\n",
        "from collections import deque\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from tf_agents.networks import q_rnn_network\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "import tensorflow as tf\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "import os\n",
        "import tempfile\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.utils import common\n",
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhUWFnbWKVlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c851045a-b687-4bf8-9710-517ebbbc3535"
      },
      "source": [
        "### reading data from csv and saving to dicts\n",
        "\n",
        "\n",
        "days_prices = defaultdict(dict)\n",
        "counter = 0\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/totrain\"\n",
        "for files in os.listdir(path):\n",
        "    counter+=1\n",
        "    np_dict = defaultdict(dict)\n",
        "    prices = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/totrain/{files}\", index_col = 0)\n",
        "    l = len(prices)\n",
        "    for k in range(0,l):\n",
        "        prices = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/totrain/{files}\", index_col = 0)\n",
        "        prices = prices.iloc[:, k:k+1]\n",
        "        for i in prices.columns:\n",
        "            prices[f\"mov_5{i}\"] = prices[f\"{i}\"].rolling(window=5).mean()\n",
        "            prices[f\"mov_20{i}\"] = prices[f\"{i}\"].rolling(window=20).mean()\n",
        "            prices[f\"mov_80{i}\"] = prices[f\"{i}\"].rolling(window=80).mean()\n",
        "            prices = prices.dropna()\n",
        "            prices[\"5-20\"] = np.round((prices[f\"mov_5{i}\"] - prices[f\"mov_20{i}\"])/prices[f\"{i}\"], 4)\n",
        "            prices[\"5-80\"] = np.round((prices[f\"mov_5{i}\"] - prices[f\"mov_80{i}\"])/prices[f\"{i}\"], 4)\n",
        "            prices[\"20-80\"] = np.round((prices[f\"mov_20{i}\"] - prices[f\"mov_80{i}\"])/prices[f\"{i}\"], 4)\n",
        "            np_dict[f\"np_diff_{i}\"] = np.array([prices[\"5-20\"].values.reshape(-1,1), prices[\"5-80\"].values.reshape(-1,1), prices[\"20-80\"].values.reshape(-1,1), prices[f\"{i}\"].values.reshape(-1,1) ])\n",
        "    days_prices[f\"np_dict_{counter}\"] = np_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV_z7so-KYjq",
        "outputId": "775c5d14-796a-43ef-f662-b03789b2f070"
      },
      "source": [
        "### defining max length of training steps\n",
        "\n",
        "\n",
        "def max_steps_lenght(days_prices):\n",
        "    max_steps = 0\n",
        "    for i in days_prices.keys():\n",
        "        crypto_dict = days_prices[i].keys()\n",
        "        number_of_crypto =  len(crypto_dict)\n",
        "        first_dict = list(crypto_dict)[0]\n",
        "        lenght = len(days_prices[i][\"np_diff_XLMUSDT\"][0])\n",
        "        lenght = lenght * (number_of_crypto - 1) # timestamp\n",
        "        max_steps += lenght\n",
        "    return max_steps\n",
        "max_steps_lenght(days_prices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "975960"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvkCaJX7VF0x"
      },
      "source": [
        "### creating environment\n",
        "\n",
        "\n",
        "class CustomEnv(py_environment.PyEnvironment):\n",
        "\n",
        "    def __init__(self, days_price, initial_balance, lookback_window_size):\n",
        "\n",
        "        self.lookback_window_size = lookback_window_size\n",
        "\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(self.lookback_window_size * 3 + 1,), dtype=np.float32, name='observation')\n",
        "\n",
        "        self.days_price = days_price\n",
        "\n",
        "        self.days_count = 1 \n",
        "\n",
        "        self.price_dict = days_price[f\"np_dict_{self.days_count}\"]\n",
        "\n",
        "        self.dict_names = list(self.price_dict.keys())\n",
        "\n",
        "        self.dict_count = 0\n",
        "\n",
        "        self.initial_balance = initial_balance\n",
        "\n",
        "        self.last_step = len(self.price_dict[f\"{self.dict_names[self.dict_count]}\"][0]) - self.lookback_window_size\n",
        "\n",
        "        self.episode_ended = False\n",
        "\n",
        "        self.active_position = 0\n",
        "\n",
        "        self.action = 0\n",
        "\n",
        "        self.fees = 0.001\n",
        "\n",
        "        self.spread = 0.001\n",
        "\n",
        "        self.money = initial_balance\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.cum_reward = 0\n",
        "\n",
        "        self.crypto_held = 0\n",
        "\n",
        "        self.previous_money = initial_balance\n",
        "\n",
        "        self._state = False\n",
        "        \n",
        "        self.price_bought = 0\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        self.episode_ended = False\n",
        "        self.current_step = 0\n",
        "        self.price_bought = 0\n",
        "        self.money = self.initial_balance\n",
        "        self.active_position = 0\n",
        "        self.action = 0\n",
        "        self.crypto_held = 0\n",
        "        self.dict_count += 1\n",
        "        if self.dict_count == len(self.dict_names):\n",
        "            self.days_count += 1\n",
        "            self.dict_count = 0\n",
        "            self.price_dict = self.days_price[f\"np_dict_{self.days_count}\"]\n",
        "            self.dict_names = list(self.price_dict.keys())\n",
        "            self.last_step = len(self.price_dict[f\"{self.dict_names[self.dict_count]}\"][0]) - self.lookback_window_size\n",
        "        self.mov_price = self.price_dict[f\"{self.dict_names[self.dict_count]}\"]\n",
        "        self.previous_money = self.initial_balance\n",
        "        self.cum_reward = 0  ###Only now\n",
        "        self.pnl = 0\n",
        "\n",
        "        self.last = self.current_step + self.lookback_window_size\n",
        "\n",
        "        self._state = np.array(\n",
        "            [self.mov_price[0, self.current_step:self.last], self.mov_price[1, self.current_step:self.last],\n",
        "             self.mov_price[2, self.current_step:self.last]])\n",
        "\n",
        "        self._state = np.append(self._state, self.active_position)\n",
        "\n",
        "        return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "    def open_position(self, action, current_price):\n",
        "        reward = 0\n",
        "        if (self.active_position == 0):\n",
        "            if action == 1:\n",
        "                self.active_position = 1\n",
        "                if self.money <= self.initial_balance * 0.1:\n",
        "                    self.episode_ended = True\n",
        "                    reward = -200\n",
        "                else:\n",
        "                    reward = 0\n",
        "                    self.money = self.money - (self.money * self.spread) - (self.money * self.fees)\n",
        "                    self.crypto_held = (self.money / current_price)\n",
        "                    self.price_bought = current_price\n",
        "                    \n",
        "        elif (self.active_position == 1):\n",
        "            if action == 0:\n",
        "                self.active_position = 0\n",
        "                self.money = self.crypto_held * current_price - (self.money * self.spread) - (self.money * self.fees)\n",
        "                reward = (current_price - self.price_bought) * self.crypto_held * 2\n",
        "                self.crypto_held = 0\n",
        "                self.price_bought = 0\n",
        "            else:\n",
        "                self.money = self.crypto_held * current_price\n",
        "                reward = 0\n",
        "\n",
        "        return reward  # think about money balance\n",
        "\n",
        "    def next_observation(self):\n",
        "\n",
        "        self.last = self.current_step + self.lookback_window_size\n",
        "        previous_price = self.mov_price[3, self.lookback_window_size + self.current_step - 1]\n",
        "        current_price = self.mov_price[3, self.lookback_window_size + self.current_step]\n",
        "        obs = np.array([self.mov_price[0, self.current_step:self.last], self.mov_price[1, self.current_step:self.last],\n",
        "                        self.mov_price[2, self.current_step:self.last]])\n",
        "        self.current_step += 1\n",
        "        return obs, current_price, previous_price \n",
        "\n",
        "    def _step(self, action):\n",
        "        reward = 0\n",
        "        done = False\n",
        "        #if self.episode_ended == True:\n",
        "        #    return self._reset()\n",
        "        obs, current_price, previous_price = self.next_observation()\n",
        "        self._state = obs\n",
        "        if (action == 1):\n",
        "            self.action = action\n",
        "            reward = self.open_position(action, current_price)\n",
        "\n",
        "        elif (action == 0):\n",
        "            if self.active_position == 0:\n",
        "                reward = -2\n",
        "            else:\n",
        "                self.action = action\n",
        "                reward = self.open_position(action, current_price)\n",
        "\n",
        "        if self.last == self.last_step-1: ##################################################################check should be if last = last.step -1\n",
        "            self.episode_ended = True\n",
        "\n",
        "        if self.episode_ended == True:\n",
        "            done = True\n",
        "            self.money = self.crypto_held * current_price - (self.money * self.spread) - (self.money * self.fees)\n",
        "            self.active_position = 0\n",
        "            reward = self.money - self.initial_balance\n",
        "\n",
        "        pnl = (self.money - self.previous_money)\n",
        "        self.previous_money = self.money\n",
        "        self.pnl += pnl\n",
        "        info = {}\n",
        "        self._state = np.append(self._state, self.active_position)\n",
        "        # return self._state, reward, done, info\n",
        "        if self.episode_ended == True:\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), float(reward))\n",
        "        else:\n",
        "            return ts.transition(\n",
        "                np.array(self._state, dtype=np.float32), reward=float(reward), discount=0)\n",
        "\n",
        "    def render(self):\n",
        "        print(f'Step: {self.current_step}, Money: {self.money}, PnL{self.pnl}, Cum reward{self.cum_reward}')\n",
        "    #####SKALIBROWAĆ REWARD\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h_7vnZeMXeB"
      },
      "source": [
        "#### creating model and agent\n",
        "\n",
        "\n",
        "class Agent_net():\n",
        "\n",
        "    def __init__(self, env):\n",
        "        # defining hyperparameters\n",
        "        self.env = env\n",
        "\n",
        "        self.train_env = tf_py_environment.TFPyEnvironment(self.env)\n",
        "        self.eval_env = tf_py_environment.TFPyEnvironment(self.env)\n",
        "        # network configuration\n",
        "        self.input_fc_layer_params = (181,)\n",
        "        self.lstm_size = (60,)\n",
        "        self.output_fc_layer_params = (60,)\n",
        "        self.learning_rate = 0.005\n",
        "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "        self.td_errors_loss_fn = common.element_wise_squared_loss\n",
        "        self.train_step_counter = tf.compat.v1.train.get_or_create_global_step() #tf.Variable(0)  \n",
        "        self.replay_buffer_max_length = 1000000\n",
        "        self.batch_size = 64\n",
        "        self.epsilon = 0.2\n",
        "\n",
        "    def qrnn(self):\n",
        "        # create a q_RNNnet\n",
        "        self.q_net = q_rnn_network.QRnnNetwork(\n",
        "            self.train_env.observation_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            input_fc_layer_params=self.input_fc_layer_params,\n",
        "            lstm_size=self.lstm_size,\n",
        "            output_fc_layer_params=self.output_fc_layer_params)\n",
        "\n",
        "    def agent_create(self):\n",
        "        self.qrnn()\n",
        "        self.target = self.qrnn()\n",
        "        self.agent = dqn_agent.DqnAgent(\n",
        "            self.train_env.time_step_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            q_network=self.q_net,\n",
        "            #target_q_network = self.target,\n",
        "            #target_update_tau = 0.1,\n",
        "            #target_update_period = 100,\n",
        "            epsilon_greedy = self.epsilon,\n",
        "            optimizer=self.optimizer,\n",
        "            td_errors_loss_fn=self.td_errors_loss_fn,\n",
        "            train_step_counter=self.train_step_counter)\n",
        "\n",
        "        self.agent.initialize() # just to start agent\n",
        "        self.eval_policy = self.agent.policy # return the policy\n",
        "        self.random_policy = random_tf_policy.RandomTFPolicy(self.train_env.time_step_spec(),\n",
        "                                                             self.train_env.action_spec())\n",
        "        return (self.agent, self.train_env, self.eval_env)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hLrybw-Nto6"
      },
      "source": [
        "def r_buffer(agent, train_env, replay_buffer_max_length):\n",
        "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "        data_spec=agent.collect_data_spec, # getting data scpecification \n",
        "        batch_size=train_env.batch_size, # how many experience there are in a batch\n",
        "        max_length=replay_buffer_max_length) # maximum capacity of buffer is max_lenght * batch size (stored in a tuple)\n",
        "    return replay_buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2lkP_j1M13C"
      },
      "source": [
        "def driver_collect(replay_buffer, train_env, agent):\n",
        "  #replay_buffer.clear()\n",
        "  observers = [replay_buffer.add_batch]       \n",
        "  driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "            train_env, agent.policy, observers, num_episodes=1)\n",
        "  driver.run()\n",
        "  return replay_buffer\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5ooTLUmVpHy"
      },
      "source": [
        "def iter_data(replay_buffer, train_env):\n",
        "  AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "  ns = replay_buffer.num_frames()\n",
        "  ns = ns.numpy()\n",
        "  dataset = replay_buffer.as_dataset(\n",
        "      num_parallel_calls=AUTOTUNE, #how many process at time, if none sequential\n",
        "      sample_batch_size=train_env.batch_size, #how many batches to get\n",
        "      num_steps=ns).prefetch(AUTOTUNE) #how many steps in a batch  \n",
        "  iterator = iter(dataset)\n",
        "  return (iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH1p3smVM5qZ"
      },
      "source": [
        "### computing reward returned in order to work collect_step() must be first\n",
        "\n",
        "\n",
        "\n",
        "def compute_avg_return(environment, policy, num_eval_episodes):\n",
        "\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_eval_episodes): # number o evaluation episodes\n",
        "\n",
        "        time_step = environment.reset() # getting first time_step\n",
        "        episode_return = 0\n",
        "        policy_state = policy.get_initial_state(batch_size=env.batch_size)\n",
        "        avg_return = 0\n",
        "        \n",
        "\n",
        "        while not time_step.is_last():\n",
        "            action_step = policy.action(time_step, policy_state) # action taken for given timestep\n",
        "            time_step = environment.step(action_step.action) # assign next time_step \n",
        "            episode_return += time_step.reward # reward taken from next time_step ?\n",
        "           \n",
        "        total_return+= episode_return\n",
        "        print(episode_return)\n",
        "    avg_return = total_return  / num_eval_episodes\n",
        "    return avg_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x737qI8cNDLq"
      },
      "source": [
        "### training\n",
        "\n",
        "\n",
        "def training(agent, train_env, replay_buffer, eval_env, iterator):\n",
        "    num_iterations = 1000000  \n",
        "    collect_steps_per_iteration = 10 \n",
        "    log_interval = 10\n",
        "    eval_interval = 50\n",
        "    num_eval_episodes = 3\n",
        "    index_out = False\n",
        "    n_episodes = 120 * len(days_prices.keys())\n",
        "    try:\n",
        "        % % time # used for prolonged training\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "    agent.train = common.function(agent.train)\n",
        "\n",
        "    # Evaluate the agent's policy once before training.\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes) #use created function\n",
        "    returns = [avg_return]\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "          try:\n",
        "            replay_buffer = driver_collect(replay_buffer, train_env, agent)\n",
        "            iterator = iter_data(replay_buffer, train_env)\n",
        "            \n",
        "\n",
        "            # Sample a batch of data from the buffer and update the agent's network.\n",
        "            experience, unused_info = iterator.get_next() \n",
        "            train_loss = agent.train(experience).loss\n",
        "          except IndexError:\n",
        "            index_out = True\n",
        "\n",
        "          step = agent.train_step_counter.numpy()\n",
        "\n",
        "          if step % log_interval == 0:\n",
        "              print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "              \n",
        "          if step % eval_interval == 0:\n",
        "              try:\n",
        "                  avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "                  print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "                  returns.append(avg_return)\n",
        "              except Exception:\n",
        "                  print(\"index error\")\n",
        "                  index_out = True\n",
        "          if index_out == True:\n",
        "            break\n",
        "\n",
        "    return agent.train_step_counter, agent, replay_buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL5D6kEc1s9f"
      },
      "source": [
        "### calling env, agent creation, training etc\n",
        "agent=None\n",
        "env = None\n",
        "tf.compat.v1.reset_default_graph()\n",
        "env = CustomEnv(days_prices, 1000, 60)\n",
        "na = Agent_net(env)\n",
        "agent, train_env, eval_env = na.agent_create()\n",
        "replay_buffer = r_buffer(agent, train_env, 50000)\n",
        "\n",
        "chkpdir = \"/content/drive/MyDrive/Colab Notebooks/policies/11.05 140k 0.01r 10batch\"\n",
        "checkpoint_dir = os.path.join(chkpdir, 'checkpoint')\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    model = na.q_net,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=agent.train_step_counter\n",
        ")\n",
        "\n",
        "\n",
        "train_checkpointer.initialize_or_restore()\n",
        "global_step = tf.compat.v1.train.get_global_step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFfPm9fvQWGN",
        "outputId": "21f6b408-bb4c-4fa7-ecbe-57b12e140140"
      },
      "source": [
        "iterator = iter_data(replay_buffer, train_env)\n",
        "save_train_step_counter, save_agent, save_replay_buffer = training(agent, train_env, replay_buffer, eval_env, iterator)\n",
        "\n",
        "\n",
        "chkpdir = \"/content/drive/MyDrive/Colab Notebooks/policies\"\n",
        "policy_dir = os.path.join(chkpdir, 'policy')\n",
        "checkpoint_dir = os.path.join(chkpdir, 'checkpoint')\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    model = na.q_net,\n",
        "    agent=save_agent,\n",
        "    policy=save_agent.policy,\n",
        "    replay_buffer=save_replay_buffer,\n",
        "    global_step=save_agent.train_step_counter\n",
        ")\n",
        "\n",
        "tf_policy_saver = policy_saver.PolicySaver(save_agent.policy)\n",
        "checkpoint = train_checkpointer.save(save_agent.train_step_counter)\n",
        "#train_checkpointer.initialize_or_restore()\n",
        "#global_step = tf.compat.v1.train.get_global_step()\n",
        "tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([-5.590033], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-5.590033], shape=(1,), dtype=float32)\n",
            "tf.Tensor([-5.590033], shape=(1,), dtype=float32)\n",
            "step = 145000: loss = 8.350639343261719\n",
            "step = 150000: loss = 5.205868244171143\n",
            "step = 155000: loss = 2.3604936599731445\n",
            "step = 160000: loss = 2.634385824203491\n",
            "step = 165000: loss = 7.562741279602051\n",
            "step = 170000: loss = 4.137465476989746\n",
            "step = 175000: loss = 6.138241767883301\n",
            "step = 180000: loss = 12.477575302124023\n",
            "step = 185000: loss = 2.565920352935791\n",
            "step = 190000: loss = 2.946113109588623\n",
            "step = 195000: loss = 4.732500076293945\n",
            "step = 200000: loss = 2.35048508644104\n",
            "tf.Tensor([3.9882019], shape=(1,), dtype=float32)\n",
            "tf.Tensor([64.43287], shape=(1,), dtype=float32)\n",
            "tf.Tensor([49.142757], shape=(1,), dtype=float32)\n",
            "step = 200000: Average Return = [39.187943]\n",
            "step = 205000: loss = 2.4197700023651123\n",
            "step = 210000: loss = 5.2478227615356445\n",
            "step = 215000: loss = 6.696335792541504\n",
            "step = 220000: loss = 1.1078574657440186\n",
            "step = 225000: loss = 9.16262149810791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as QRnnNetwork_layer_call_and_return_conditional_losses, QRnnNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dynamic_unroll_1_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as QRnnNetwork_layer_call_and_return_conditional_losses, QRnnNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dynamic_unroll_1_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/policies/policy/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/policies/policy/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTowIFWSy-6D",
        "outputId": "9d61112b-5cf2-46fb-c612-ff536b6683c9"
      },
      "source": [
        "  chkpdir = \"/content/drive/MyDrive/Colab Notebooks/policies\"\n",
        "  policy_dir = os.path.join(chkpdir, 'policy')\n",
        "  checkpoint_dir = os.path.join(chkpdir, 'checkpoint')\n",
        "  train_checkpointer = common.Checkpointer(\n",
        "      ckpt_dir=checkpoint_dir,\n",
        "      max_to_keep=1,\n",
        "      model = na.q_net,\n",
        "      agent=agent,\n",
        "      policy=agent.policy,\n",
        "      replay_buffer=replay_buffer,\n",
        "      global_step=agent.train_step_counter\n",
        "  )\n",
        "\n",
        "  tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "  checkpoint = train_checkpointer.save(agent.train_step_counter)\n",
        "  #train_checkpointer.initialize_or_restore()\n",
        "  #global_step = tf.compat.v1.train.get_global_step()\n",
        "  tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as QRnnNetwork_layer_call_and_return_conditional_losses, QRnnNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dynamic_unroll_40_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as QRnnNetwork_layer_call_and_return_conditional_losses, QRnnNetwork_layer_call_fn, EncodingNetwork_layer_call_and_return_conditional_losses, EncodingNetwork_layer_call_fn, dynamic_unroll_40_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/policies/policy/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/policies/policy/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmtyIJhjHtnO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}